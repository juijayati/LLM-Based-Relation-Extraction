{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER and RE using LLM and RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the envirnment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = open('langchain-api-key.txt', 'r').read()\n",
    "os.environ['OPENAI_API_KEY'] = open('openai-api-key-pitt.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import re\n",
    "import urllib\n",
    "from time import sleep\n",
    "import requests\n",
    "from xml.etree import ElementTree\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tiktoken \n",
    "import openai\n",
    "from datetime import datetime\n",
    "from pronto import Ontology\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "gpt3 = \"gpt-3.5-turbo-0125\"\n",
    "gpt4 = \"gpt-4-0125-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process GO File\n",
    "Read the go-basic.obo file and convert to a dict for indexin with Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30655\n",
      "Details for GO:0006915:\n",
      "id: GO:0006915\n",
      "name: apoptotic process\n",
      "namespace: biological_process\n",
      "definition: A programmed cell death process which begins when a cell receives an internal (e.g. DNA damage) or external signal (e.g. an extracellular death ligand), and proceeds through a series of biochemical events (signaling pathway phase) which trigger an execution phase. The execution phase is the last step of an apoptotic process, and is typically characterized by rounding-up of the cell, retraction of pseudopodes, reduction of cellular volume (pyknosis), chromatin condensation, nuclear fragmentation (karyorrhexis), plasma membrane blebbing and fragmentation of the cell into apoptotic bodies. When the execution phase is completed, the cell has died.\n",
      "is_a: apoptotic process,\n",
      "programmed cell death,\n",
      "cell death,\n",
      "cellular process,\n",
      "biological_process\n",
      "synonyms: apoptosis,\n",
      "apoptotic programmed cell death,\n",
      "apoptotic program,\n",
      "apoptosis activator activity,\n",
      "apoptosis signaling,\n",
      "caspase-dependent programmed cell death,\n",
      "cellular suicide,\n",
      "commitment to apoptosis,\n",
      "induction of apoptosis,\n",
      "type I programmed cell death,\n",
      "activation of apoptosis,\n",
      "apoptotic cell death,\n",
      "cell suicide,\n",
      "signaling (initiator) caspase activity,\n",
      "induction of apoptosis by p53,\n",
      "programmed cell death by apoptosis\n"
     ]
    }
   ],
   "source": [
    "def parse_go_file(file_path):\n",
    "    # Load the ontology from a file\n",
    "    ont = Ontology(file_path)\n",
    "\n",
    "    # Initialize a dictionary to hold the GO terms\n",
    "    go_dict = {}\n",
    "\n",
    "    # Iterate through each term in the ontology\n",
    "    for term in ont.terms():\n",
    "        if term.namespace == \"biological_process\":\n",
    "            go_dict[term.id] = {\n",
    "                'id': term.id,\n",
    "                'name': term.name,\n",
    "                'namespace': term.namespace,\n",
    "                'definition': term.definition,\n",
    "                'is_a': ',\\n'.join([parent.name for parent in term.superclasses()]),\n",
    "                'synonyms': ',\\n'.join([syn.description for syn in term.synonyms])\n",
    "            }\n",
    "            \n",
    "    return go_dict\n",
    "\n",
    "# Path to your GO file\n",
    "go_file_path = './Ontology/go-basic.obo'\n",
    "\n",
    "# Parse the GO file and convert to a dictionary\n",
    "go_dict = parse_go_file(go_file_path)\n",
    "\n",
    "print(len(go_dict))\n",
    "\n",
    "# Example: print details of a specific GO term\n",
    "go_id = 'GO:0006915'  # Example GO ID\n",
    "if go_id in go_dict:\n",
    "    print(f\"Details for {go_id}:\")\n",
    "    for key, value in go_dict[go_id].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(f\"{go_id} not found in the GO dictionary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the GO Biological Processes with Chroma \n",
    "For implementating RAG, we need to index the go_dict object with Chroma. This will help LLM for the annotation of Biological Processes.\n",
    "We'll save the vector database locally and load from the local store for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 47s, sys: 7.02 s, total: 2min 54s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ids = list(go_dict.keys())\n",
    "docs = [v.get('name', '') for v in go_dict.values()]\n",
    "metadatas = [v for v in go_dict.values()]\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_texts(ids=ids,\n",
    "                                texts=docs, \n",
    "                                metadatas=metadatas,\n",
    "                                collection_name='GO_BP',\n",
    "                                embedding=OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the GO Biological Processes with BM25 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 749 ms, sys: 72 ms, total: 821 ms\n",
      "Wall time: 814 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ids = list(go_dict.keys())\n",
    "docs = [v.get('name', '') for v in go_dict.values()]\n",
    "metadatas = [v for v in go_dict.values()]\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_texts(texts = docs, metadatas = metadatas)\n",
    "bm25_retriever.k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'text'], template='\\nFollowing is the list of candidate GO biological process concepts:\\n\\n{context}\\n\\nYour job is to parse the following title and identify all instances of the same, or equivalent, or similar biological process concepts given the above list of candidate concepts.\\n\\nMark up the concepts (if any) in double square brackets preserving the original text of the title inside the brackets like [[original text]].\\n\\nIf no biological process is explicitly mentioned in the supplied text, return the supplied text unchanged.\\n\\n\\nExample:\\ntext: \"Interference with KCNJ2 inhibits proliferation, migration and EMT progression of papillary thyroid carcinoma cells by upregulating GNG2 expression.\"\\noutput: \"Interference with KCNJ2 inhibits [[proliferation]], [[migration]] and EMT progression of papillary thyroid carcinoma cells by upregulating GNG2 expression.\"\\n\\n\\n\\ntitle:{text}\\noutput: \\n'))]\n"
     ]
    }
   ],
   "source": [
    "# define the prompt templates for NER\n",
    "ner_template = \"\"\"\n",
    "Following is the list of candidate GO biological process concepts:\\n\n",
    "{context}\\n\n",
    "Your job is to parse the following title and identify all instances of the same, or equivalent, or similar biological process concepts given the above list of candidate concepts.\\n\n",
    "Mark up the concepts (if any) in double square brackets preserving the original text of the title inside the brackets like [[original text]].\\n\n",
    "If no biological process is explicitly mentioned in the supplied text, return the supplied text unchanged.\\n\\n\n",
    "Example:\n",
    "text: \"Interference with KCNJ2 inhibits proliferation, migration and EMT progression of papillary thyroid carcinoma cells by upregulating GNG2 expression.\"\n",
    "output: \"Interference with KCNJ2 inhibits [[proliferation]], [[migration]] and EMT progression of papillary thyroid carcinoma cells by upregulating GNG2 expression.\"\n",
    "\\n\\n\n",
    "title:{text}\n",
    "output: \n",
    "\"\"\"\n",
    "\n",
    "ner_prompt = ChatPromptTemplate.from_template(ner_template)\n",
    "print(ner_prompt)\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=gpt4, temperature=0)\n",
    "\n",
    "# initialize the ector_retriver\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 50})\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever], weights=[1.0, 1.0]\n",
    ")\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    print(\"\\n\".join(doc.page_content for doc in docs))\n",
    "    return \"\\n\".join(doc.page_content for doc in docs)\n",
    "    #return \"fibroblast proliferation, angiogenesis, wound healing, cell proliferation\"\n",
    "\n",
    "ner_rag_chain = (\n",
    "    {\"context\": ensemble_retriever | format_docs,\\\n",
    "     \"text\": RunnablePassthrough()}\n",
    "    | ner_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER using GPT-4 and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [01:48<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "gpt_annots = {}\n",
    "\n",
    "gs_annots = json.load(open('gs_annots.json', 'r'))\n",
    "\n",
    "for item in tqdm(gs_annots['documents']):\n",
    "    pmid = item['pmid']\n",
    "    title = item['title']\n",
    "\n",
    "    response = ner_rag_chain.invoke(title)\n",
    "    #matches = re.findall(annot_pattern, response)\n",
    "\n",
    "    #if len(matches) > 0:\n",
    "    gpt_annots[pmid] = response\n",
    "\n",
    "       \n",
    "df = pd.DataFrame.from_dict(gpt_annots, orient='index')\n",
    "df.to_csv('gpt4_annots.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT annotations and concept grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:00<00:00, 2818.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "with open('gs_annots_6.json', 'r') as f:\n",
    "    gs_annots = json.load(f)\n",
    "gpt_annots = pd.read_csv('gpt4_annots.csv')\n",
    "\n",
    "gpt_annots_extended = {'documents':[]}\n",
    "annot_pattern = r\"\\[\\[([^\\]]+)\\]\\]\"\n",
    "for item in tqdm(gs_annots['documents']):\n",
    "    pmid = item['pmid']\n",
    "    title = item['title']\n",
    "    annotations = []\n",
    "    for a_item in item['annotations']:\n",
    "        if a_item['type'] != 'Biological Process':\n",
    "            annotations.append(a_item)\n",
    "    annot_sent = gpt_annots[gpt_annots['pmid'] == pmid]['annotation'].values[0]\n",
    "    assert title == annot_sent.replace('[[', '').replace(']]', '')\n",
    "    i = 0\n",
    "    for match in re.finditer(annot_pattern, annot_sent):\n",
    "        start, end = match.span()\n",
    "        start = start - i\n",
    "        end = end - (i + 4)\n",
    "        annotations.append({'entity':title[start:end], 'type': 'Biological Process', 'kb_concept_name':'',\\\n",
    "                            'kb_id': '', 'start': start, 'end': end})\n",
    "        #print(annotations)\n",
    "        i = i + 4\n",
    "    gpt_annots_extended['documents'].append({'pmid':pmid, 'title':title, 'annotations':annotations})\n",
    "\n",
    "    \n",
    "with open('gpt4_annots_2.json', 'w') as f:\n",
    "    json.dump(gpt_annots_extended, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept Grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [03:09<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "ground_prompt_template = \"\"\"\n",
    "Your role is to assign a concept ID that best matches the supplied text, using the supplied list of candidate concepts.\n",
    "Return as a string \"CONCEPT_NAME|CONCEPT_ID|Score\" triple where the score represents the confidence \n",
    "of the assignment and should be between 0 and 1.\n",
    "Only use concept IDs from the supplied list of candidate concepts.\n",
    "Only return a row if the concept ID is a match for the input text.\n",
    "If there is no match, return NOT FOUND.\\n\n",
    "Here are the candidate concepts, as CONCEPT_NAME|CONCEPT_ID pairs:\n",
    "{candidates}\n",
    "The overall context for this is the sentence: {context}\n",
    "Concept to ground: {text}\n",
    "\"\"\"\n",
    "\n",
    "ground_prompt = ChatPromptTemplate.from_template(ground_prompt_template)\n",
    "#print(prompt)\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    #print(\"\\n\".join(doc.metadata['name']+'//'+doc.metadata['id'] for doc in docs))\n",
    "    return (\"\\n\".join(doc.metadata['name']+'|'+doc.metadata['id'] for doc in docs))\n",
    "    #return \"fibroblast proliferation, angiogenesis, wound healing, cell proliferation\"\n",
    "\n",
    "rag_chain = (\n",
    "    {\"candidates\": itemgetter(\"text\") | vector_retriever | format_docs,\\\n",
    "     \"context\": itemgetter(\"context\"), \\\n",
    "     \"text\": itemgetter(\"text\")}\n",
    "    | ground_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = rag_chain.invoke({'context':\"Nicotinamide inhibits corneal endothelial mesenchymal transition and accelerates wound healing.\",\\\n",
    "                             'text': 'corneal endothelial mesenchymal transition'})\n",
    "#print(response)\n",
    "# print({'kb_concept_name':response.split('|')[0], 'kb_id':response.split('|')[1], \n",
    "#        'confidence_score':response.split('|')[2]})\n",
    "\n",
    "with open('gpt4_annots_2.json', 'r') as f:\n",
    "    gpt_annots = json.load(f)\n",
    "gpt_annots_new = {'documents':[]}\n",
    "for item in tqdm(gpt_annots['documents']):\n",
    "    new_item = {'pmid':item['pmid'], 'title':item['title'], 'annotations':[]}\n",
    "    for ann in item['annotations']:\n",
    "        if ann['type'] == 'Biological Process':\n",
    "            text = ann['entity']\n",
    "            response = rag_chain.invoke({'context': item['title'], 'text': text})\n",
    "            if \"NOT FOUND\" not in response:\n",
    "                ann['kb_concept_name'] = response.split('|')[0]\n",
    "                ann['kb_id'] = response.split('|')[1]\n",
    "        new_item['annotations'].append(ann)\n",
    "    gpt_annots_new['documents'].append(item)\n",
    "    #break\n",
    "with open('gpt4_annots_3.json', 'w') as f:\n",
    "    json.dump(gpt_annots_new, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [03:56<00:00,  2.27s/it]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "re_prompt_template = \"\"\"{context}\n",
    "Based on the above statement only, your job is to label the following sentences as true, false, or unknown if there's a strong evidence for the regulatory relationship between the two concepts in the above statement.\n",
    "and provide a chain-of-thought (CoT) supporting the answer. \n",
    "The answers should be in the following format.\n",
    "1. label//CoT\n",
    "2. label//CoT\n",
    "3. label//Cot\n",
    "..... etc.\n",
    "{candidates}\n",
    "\"\"\"\n",
    "\n",
    "re_prompt = ChatPromptTemplate.from_template(re_prompt_template)\n",
    "#print(prompt)\n",
    "\n",
    "# retriever\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 50})\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    #print(\"\\n\".join(doc.metadata['name']+'//'+doc.metadata['id'] for doc in docs))\n",
    "    return (\"\\n\".join(doc.metadata['name']+'//'+doc.metadata['id'] for doc in docs))\n",
    "    #return \"fibroblast proliferation, angiogenesis, wound healing, cell proliferation\"\n",
    "\n",
    "rag_chain_re = (\n",
    "    {\"context\": itemgetter(\"context\"), \"candidates\": itemgetter(\"candidates\")}\n",
    "    | re_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "gpt_annots = json.load(open('gpt4_annots_3.json','r'))\n",
    "gpt_annots_new = {'documents': []}\n",
    "def generate_candidate_sentences(annotations):\n",
    "    sources = []\n",
    "    targets = []\n",
    "    for item in annotations:\n",
    "        if item['type'] == 'Biological Process':\n",
    "            targets.append(item['entity'])\n",
    "        else:\n",
    "            sources.append(item['entity'])\n",
    "    if not sources:\n",
    "        return None\n",
    "    if not targets:\n",
    "        return None\n",
    "    candidates = []\n",
    "    i = 1\n",
    "    for s in sources:\n",
    "        for t in targets:\n",
    "            candidates.append(f\"{i}. {s} upregulates {t}\")\n",
    "            i = i + 1\n",
    "            candidates.append(f\"{i}. {s} downregulates {t}\")\n",
    "            i = i + 1\n",
    "    return '\\n'.join(candidates)\n",
    "\n",
    "def extract_relations(candidate, response):\n",
    "    relations = []\n",
    "    responses = response.splitlines()\n",
    "    sents = candidates.splitlines()\n",
    "    sents = [re.sub(r\"(\\d+)\\. \", '', sent) for sent in sents]\n",
    "    for sent, response in zip(sents, responses):\n",
    "        if \"true\" in response.split('//')[0].lower():\n",
    "            cot = response.split('//')[1]\n",
    "            if 'upregulates' in sent:\n",
    "                rel_type = 'positive'\n",
    "                sent = sent.replace(' upregulates ', '||')\n",
    "            else:\n",
    "                rel_type = 'negative'\n",
    "                sent = sent.replace(' downregulates ', '||')\n",
    "            relations.append({'source':sent.split('||')[0], 'target': sent.split('||')[1], \\\n",
    "                              'relation': rel_type, 'chain-of-thought': cot})\n",
    "    return relations\n",
    "\n",
    "for item in tqdm(gpt_annots['documents']):\n",
    "    title = item['title']\n",
    "    annotations = item['annotations']\n",
    "    candidates = generate_candidate_sentences(annotations)\n",
    "    if candidates is None:\n",
    "        item['relations'] = []\n",
    "        gpt_annots_new['documents'].append(item)\n",
    "        continue\n",
    "    response = rag_chain_re.invoke({'context': title, 'candidates': candidates})\n",
    "    item['relations'] = extract_relations(candidates, response)\n",
    "    gpt_annots_new['documents'].append(item)\n",
    "\n",
    "\n",
    "with open('gpt4_annots_4.json', 'w') as f:\n",
    "    json.dump(gpt_annots_new, f, indent = 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-Based NER, Grounding and RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2\n",
      "For maximum performance, you can install NMSLIB from sources \n",
      "pip install --no-binary :all: nmslib\n",
      "/afs/cs.pitt.edu/usr0/jaj146/miniconda3/envs/aime2024/lib/python3.9/site-packages/spacy/language.py:2141: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n",
      "/afs/cs.pitt.edu/usr0/jaj146/miniconda3/envs/aime2024/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.1.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/afs/cs.pitt.edu/usr0/jaj146/miniconda3/envs/aime2024/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.1.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epithelial-mesenchymal transition C1523298 {'kb': 'GO', 'term_type': 'PT', 'kb_cui': 'GO:0001837', 'kb_name': 'epithelial to mesenchymal transition'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.linking import EntityLinker\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from spacy.tokens import Span\n",
    "from spacy.pipeline import merge_entities\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "class SpacyExtractor:\n",
    "    def __init__(self, model_name = \"en_core_sci_sm\", umls_cuis_path = \"umls_cuis_go.pkl\"):  \n",
    "        self.nlp = self.load_model(model_name)\n",
    "        self.tuis = pd.read_csv(\"umls_tuis.txt\").set_index('tui')['label'].to_dict() \n",
    "        self.umls_cuis = pickle.load(open(umls_cuis_path, 'rb')) \n",
    "        Span.set_extension(\"cui\", default='', force=True)\n",
    "        Span.set_extension(\"tuis\", default=[], force=True)\n",
    "        Span.set_extension(\"mapping\", default=[], force=True)  \n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        self.nlp = spacy.load(model_name)\n",
    "        # Add the abbreviation pipe to the spacy pipeline.\n",
    "        #self.nlp.add_pipe(\"abbreviation_detector\")\n",
    "        # that linking will only be performed on the long form of abbreviations.\n",
    "        self.nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\", \"threshold\":0.7})\n",
    "        # add merge_entities pipe for RE\n",
    "        self.nlp.add_pipe(\"merge_entities\")\n",
    "        return self.nlp\n",
    "    \n",
    "    def expand_entities(self, doc):\n",
    "        linker = self.nlp.get_pipe(\"scispacy_linker\")\n",
    "        ents = list(doc.ents)\n",
    "        allowed_tuis = self.tuis.keys()\n",
    "        new_ents = []\n",
    "        for i in range(len(ents)):\n",
    "            old_ent = ents[i]\n",
    "            if old_ent._.kb_ents:\n",
    "                umls_ent = linker.kb.cui_to_entity[old_ent._.kb_ents[0][0]]\n",
    "                cui = umls_ent.concept_id\n",
    "                # if cui in list(self.umls_cuis.keys()):\n",
    "                #     mappings = self.umls_cuis[cui]\n",
    "                # else:\n",
    "                mapping = self.umls_cuis.get(cui, None)\n",
    "                tuis = list([self.tuis[t] for t in umls_ent.types if t in allowed_tuis])\n",
    "                if mapping is not None:\n",
    "                    label = '[' + ', '.join(set(tuis)) + ']'\n",
    "                    new_ent = Span(doc, old_ent.start, old_ent.end, label = label)\n",
    "                    new_ent._.set('cui', cui)\n",
    "                    #new_ent._.set('tuis', tuis)\n",
    "                    new_ent._.set('mapping', mapping)\n",
    "                    new_ents.append(new_ent)\n",
    "\n",
    "        doc.ents = new_ents\n",
    "        return doc\n",
    "umls_cuis_go = pickle.load(open('umls_cuis_go.pkl', 'rb'))\n",
    "extractor = SpacyExtractor()\n",
    "doc = extractor.nlp(\"microRNA-23b suppresses epithelial-mesenchymal transition (EMT) and metastasis in hepatocellular carcinoma via targeting Pyk2.\")\n",
    "doc = extractor.expand_entities(doc)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent._.cui, ent._.mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### identify GO biological process concepts in the text\n",
    "import json\n",
    "\n",
    "with open('gs_annots_6.json', 'r') as f:\n",
    "    gs_annots = json.load(f)\n",
    "\n",
    "spacy_annots = {'documents':[]}\n",
    "\n",
    "for item in gs_annots['documents']:\n",
    "    title = item['title']\n",
    "    pmid = item['pmid']\n",
    "    annotations = item['annotations']\n",
    "    new_annotations = []\n",
    "    for ann in annotations:\n",
    "        if ann['type'] != 'Biological Process':\n",
    "            new_annotations.append(ann)\n",
    "    doc = extractor.nlp(title)\n",
    "    doc = extractor.expand_entities(doc)\n",
    "    for ent in doc.ents:\n",
    "        if ent._.mapping:\n",
    "            new_annotations.append({'entity': ent.text, 'type': 'Biological Process', 'kb_concept_name':ent._.mapping['kb_name'],\\\n",
    "                        'kb_id': ent._.mapping['kb_cui'], 'start': ent.start_char, 'end': ent.end_char})\n",
    "    spacy_annots['documents'].append({'pmid':pmid, 'title':title, 'annotations':new_annotations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spacy_annots.json', 'w') as f:\n",
    "    json.dump(spacy_annots, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based RE using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rules import *\n",
    "from spacy.util import filter_spans\n",
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "\n",
    "\n",
    "def assign_rules_to_matcher(matcher, ent_types):\n",
    "    dep_pattern1 = [{'RIGHT_ID': 'verb', 'RIGHT_ATTRS': {'DEP': {'IN':['ROOT','ccomp']},'POS': 'VERB', \n",
    "                                                     'LEMMA': {'IN':rule_1_triggers}}},\n",
    "               {'LEFT_ID': 'verb', 'REL_OP': '>', \n",
    "                'RIGHT_ID': 'subject', \n",
    "                'RIGHT_ATTRS': {'DEP': 'nsubj','ENT_TYPE': {'IN': ent_types}}},\n",
    "               {'LEFT_ID': 'verb', 'REL_OP': '>', 'RIGHT_ID': 'd_object', \n",
    "                'RIGHT_ATTRS': {'DEP': 'dobj', 'ENT_TYPE': {'IN': ent_types}}}]\n",
    "    # matches ===> x promotes y and z; we found that x promotes y and z\n",
    "    dep_pattern2 = [{'RIGHT_ID': 'verb', 'RIGHT_ATTRS': {'DEP': {'IN':['ROOT','ccomp']},'POS': 'VERB',\n",
    "                                                        'LEMMA': {'IN':rule_1_triggers}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>', \n",
    "                    'RIGHT_ID': 'subject', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'nsubj','ENT_TYPE': {'IN': ent_types}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '.', 'RIGHT_ID': 'd_object', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'conj', 'ENT_TYPE': {'IN': ent_types}}}]\n",
    "\n",
    "    # matches ===> x ....., and supresses w and z\n",
    "    dep_pattern3 = [{'RIGHT_ID': 'verb', 'RIGHT_ATTRS': {'DEP': {'IN':['conj','advcl']},'POS': 'VERB',\n",
    "                                                        'LEMMA': {'IN':rule_1_triggers}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '$--', \n",
    "                    'RIGHT_ID': 'subject', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'nsubj','ENT_TYPE': {'IN': ent_types}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>>', 'RIGHT_ID': 'd_object', \n",
    "                    'RIGHT_ATTRS': {'DEP': {'IN':['dobj','conj']}, 'ENT_TYPE': {'IN': ent_types}}}\n",
    "                    ]\n",
    "\n",
    "    # matches ===> x causes upregulation of y; we found that x causes upregulation of y\n",
    "    dep_pattern4 = [{'RIGHT_ID': 'verb', 'RIGHT_ATTRS': {'DEP': {'IN':['ROOT','ccomp']},'POS': 'VERB',\n",
    "                                                        'LEMMA': {'IN':rule_2_verb_triggers}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>', \n",
    "                    'RIGHT_ID': 'subject', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'nsubj','ENT_TYPE': {'IN': ent_types}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>', 'RIGHT_ID': 'effect', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'dobj', 'LEMMA': {'IN':rule_2_effect_triggers}}},\n",
    "                   {'LEFT_ID': 'effect', 'REL_OP': '>', 'RIGHT_ID': 'd_object', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'nmod', 'ENT_TYPE': {'IN': ent_types},\n",
    "                                    'LEMMA': {'NOT_IN':rule_2_effect_triggers}}}]\n",
    "\n",
    "    # matches ===> x causes upregulation of y and z; we found that x causes upregulation of y and z\n",
    "    dep_pattern5 = [{'RIGHT_ID': 'verb', 'RIGHT_ATTRS': {'DEP': {'IN':['ROOT','ccomp']},'POS': 'VERB',\n",
    "                                                        'LEMMA': {'IN':rule_2_verb_triggers}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>', \n",
    "                    'RIGHT_ID': 'subject', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'nsubj','ENT_TYPE': {'IN': ent_types}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>', 'RIGHT_ID': 'effect', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'dobj', 'LEMMA': {'IN':rule_2_effect_triggers}}},\n",
    "                   {'LEFT_ID': 'effect', 'REL_OP': '>>', 'RIGHT_ID': 'd_object', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'conj', 'ENT_TYPE': {'IN': ent_types},\n",
    "                                    'LEMMA': {'NOT_IN':rule_2_effect_triggers}}}]\n",
    "\n",
    "    # matches ===> x ....., and downregulation of z and k\n",
    "    dep_pattern6 = [{'RIGHT_ID': 'verb', 'RIGHT_ATTRS': {'DEP': {'IN':['ROOT','ccomp']},'POS': 'VERB',\n",
    "                                                        'LEMMA': {'IN':rule_2_verb_triggers}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>', \n",
    "                    'RIGHT_ID': 'subject', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'nsubj','ENT_TYPE': {'IN': ent_types}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>>', 'RIGHT_ID': 'effect', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'conj', 'LEMMA': {'IN':rule_2_effect_triggers}}},\n",
    "                   {'LEFT_ID': 'effect', 'REL_OP': '>>', 'RIGHT_ID': 'd_object', \n",
    "                    'RIGHT_ATTRS': {'DEP': {'IN':['nmod','conj']}, 'ENT_TYPE': {'IN': ent_types},\n",
    "                                   'LEMMA': {'NOT_IN':rule_2_effect_triggers}}}]\n",
    "\n",
    "    # matches ===> y is promoted by x\n",
    "    dep_pattern7 = [{'RIGHT_ID': 'verb', 'RIGHT_ATTRS': {'DEP': {'IN':['ROOT','ccomp']},'POS': 'VERB', \n",
    "                                                         'LEMMA': {'IN':rule_1_triggers}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>', \n",
    "                    'RIGHT_ID': 'subject', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'nmod','ENT_TYPE': {'IN': ent_types}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>', 'RIGHT_ID': 'd_object', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'nsubjpass', 'ENT_TYPE': {'IN': ent_types}}}]\n",
    "\n",
    "    # matches ===> .... and z is suppressed by x\n",
    "    dep_pattern8 = [{'RIGHT_ID': 'verb', 'RIGHT_ATTRS': {'DEP': 'conj','POS': 'VERB',\n",
    "                                                        'LEMMA': {'IN':rule_1_triggers}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>', \n",
    "                    'RIGHT_ID': 'subject', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'nmod','ENT_TYPE': {'IN': ent_types}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>', 'RIGHT_ID': 'd_object', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'nsubjpass', 'ENT_TYPE': {'IN': ent_types}}}]\n",
    "\n",
    "    # matches ===> upregulation of y is caused by x; we found that upregulation of y is caused by x\n",
    "    dep_pattern9 = [{'RIGHT_ID': 'verb', 'RIGHT_ATTRS': {'DEP': {'IN':['ROOT','ccomp']},'POS': 'VERB',\n",
    "                                                        'LEMMA': {'IN':rule_2_verb_triggers}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>', \n",
    "                    'RIGHT_ID': 'subject', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'nmod','ENT_TYPE': {'IN': ent_types}}},\n",
    "                   {'LEFT_ID': 'verb', 'REL_OP': '>', 'RIGHT_ID': 'effect', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'nsubjpass', 'LEMMA': {'IN':rule_2_effect_triggers}}},\n",
    "                   {'LEFT_ID': 'effect', 'REL_OP': '>', 'RIGHT_ID': 'd_object', \n",
    "                    'RIGHT_ATTRS': {'DEP': 'nmod', 'ENT_TYPE': {'IN': ent_types},\n",
    "                                    'LEMMA': {'NOT_IN':rule_2_effect_triggers}}}]\n",
    "\n",
    "    # matches ===> x is an inhibitor of y\n",
    "\n",
    "    # Add the pattern to the matcher under the name 'nsubj_verb'\n",
    "    matcher.add('rule_1', patterns=[dep_pattern1, dep_pattern3, dep_pattern7, dep_pattern8])\n",
    "    matcher.add('rule_2', patterns=[dep_pattern4, dep_pattern5, dep_pattern6, dep_pattern9])\n",
    "    return matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:01<00:00, 53.26it/s]\n"
     ]
    }
   ],
   "source": [
    "### read from spacy_annots.json and create new entities\n",
    "spacy_annots = json.load(open('spacy_annots.json', 'r'))\n",
    "\n",
    "spacy_annots_new = {'documents':[]}\n",
    "\n",
    "for item in tqdm(spacy_annots['documents']):\n",
    "    title = item['title']\n",
    "    pmid = item['pmid']\n",
    "    annotations = item['annotations']\n",
    "    doc = extractor.nlp(title.lower())\n",
    "    new_ents = []\n",
    "    item['relations'] = []\n",
    "    #print(doc)\n",
    "    doc.ents = []\n",
    "    for ann in annotations:\n",
    "        #print(ann)\n",
    "        new_ent = doc.char_span(int(ann['start']), int(ann['end']), label = ann['type'], kb_id=ann['kb_id'])\n",
    "        if new_ent is not None:\n",
    "            new_ents.append(new_ent)\n",
    "\n",
    "    if not new_ents:\n",
    "        spacy_annots_new['documents'].append(item)\n",
    "        continue\n",
    "    \n",
    "    doc.ents = filter_spans(new_ents)\n",
    "    dep_matcher = DependencyMatcher(vocab=extractor.nlp.vocab)\n",
    "    ent_types = list(set([ent.label_ for ent in doc.ents if ent.label_ != \"ENTITY\"]))\n",
    "    \n",
    "    dep_matcher = assign_rules_to_matcher(dep_matcher, ent_types)\n",
    "    \n",
    "    dep_matches = dep_matcher(doc)\n",
    "\n",
    "    for match in dep_matches:\n",
    "\n",
    "        # Take the first item in the tuple at [0] and assign it under\n",
    "        # the variable 'pattern_name'. This item is a spaCy Lexeme object.\n",
    "        pattern_name = extractor.nlp.vocab[match[0]].text\n",
    "        #print(pattern_name)\n",
    "\n",
    "        matches = match[1]\n",
    "\n",
    "        if pattern_name == \"rule_1\":\n",
    "            relation, subject, dobject = doc[matches[0]], doc[matches[1]], doc[matches[2]]\n",
    "\n",
    "\n",
    "        elif pattern_name == \"rule_2\":\n",
    "            subject, relation, dobject = doc[matches[1]], doc[matches[2]], doc[matches[3]]\n",
    "\n",
    "        if dobject.ent_type_ != 'Biological Process':\n",
    "            continue\n",
    "        if subject.ent_type_ == 'Biological Process':\n",
    "            continue\n",
    "\n",
    "        item['relations'].append({'source':subject.text, 'target':dobject.text, 'relation':relation_triggers[relation.lemma_]})\n",
    "    \n",
    "    spacy_annots_new['documents'].append(item)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spacy_annots_3.json', 'w') as f:\n",
    "    json.dump(spacy_annots_new, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION of SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy Precision (NER):  0.7151515151515152\n",
      "Spacy Recall (NER):  0.5784313725490197\n",
      "Spacy F1: (NER) 0.6395663956639567\n"
     ]
    }
   ],
   "source": [
    "# NER evaluation\n",
    "gs_annots = json.load(open('gs_annots_6.json', 'r'))\n",
    "spacy_annots = json.load(open('spacy_annots_3.json', 'r'))\n",
    "tp=0\n",
    "tp_fp=0\n",
    "tp_fn=0\n",
    "\n",
    "for gs_item, spacy_item in zip(gs_annots['documents'], spacy_annots['documents']):\n",
    "    assert gs_item['pmid'] == spacy_item['pmid']\n",
    "    gs_annotation = gs_item['annotations']\n",
    "    spacy_annotation = spacy_item['annotations']\n",
    "\n",
    "    for gs_ann in gs_annotation:\n",
    "        if gs_ann['type'] == 'Biological Process':\n",
    "            for spacy_ann in spacy_annotation:\n",
    "                if spacy_ann['type'] == 'Biological Process':\n",
    "                    if gs_ann['entity'].lower() in spacy_ann['entity'].lower():\n",
    "                        tp = tp + 1\n",
    "\n",
    "for gs_item in gs_annots['documents']:\n",
    "    gs_annotation = gs_item['annotations']\n",
    "    for gs_ann in gs_annotation:\n",
    "        if gs_ann['type'] == 'Biological Process':\n",
    "            tp_fn = tp_fn + 1\n",
    "\n",
    "for spacy_item in spacy_annots['documents']:\n",
    "    spacy_annotation = spacy_item['annotations']\n",
    "    for spacy_ann in spacy_annotation:\n",
    "        if spacy_ann['type'] == 'Biological Process':\n",
    "            tp_fp = tp_fp + 1\n",
    "\n",
    "print('Spacy Precision (NER): ', tp/tp_fp)\n",
    "print('Spacy Recall (NER): ', tp/tp_fn)\n",
    "print('Spacy F1: (NER)', 2*tp/(tp_fp + tp_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [00:00, 212369.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy Precision (GR):  0.5757575757575758\n",
      "Spacy Recall (GR):  0.46568627450980393\n",
      "Spacy F1 (GR):  0.5149051490514905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GR evaluation\n",
    "gs_annots = json.load(open('gs_annots_6.json', 'r'))\n",
    "spacy_annots = json.load(open('spacy_annots_3.json', 'r'))\n",
    "\n",
    "tp_fp = 0\n",
    "tp = 0\n",
    "tp_fn = 0\n",
    "\n",
    "for gs_item, spacy_item in tqdm(zip(gs_annots['documents'], spacy_annots['documents'])):\n",
    "    assert gs_item['pmid'] == spacy_item['pmid']\n",
    "    gs_annotation = gs_item['annotations']\n",
    "    spacy_annotation = spacy_item['annotations']\n",
    "\n",
    "    for gs_ann in gs_annotation:\n",
    "        if gs_ann['type'] == 'Biological Process':\n",
    "            for spacy_ann in spacy_annotation:\n",
    "                if spacy_ann['type'] == 'Biological Process':\n",
    "                    if gs_ann['entity'].lower() in spacy_ann['entity'].lower() and gs_ann['kb_id'] == spacy_ann['kb_id']:\n",
    "                        tp = tp + 1\n",
    "\n",
    "for gs_item in gs_annots['documents']:\n",
    "    gs_annotation = gs_item['annotations']\n",
    "    for gs_ann in gs_annotation:\n",
    "        if gs_ann['type'] == 'Biological Process':\n",
    "            tp_fn = tp_fn + 1\n",
    "\n",
    "for spacy_item in spacy_annots['documents']:\n",
    "    spacy_annotation = spacy_item['annotations']\n",
    "    for spacy_ann in spacy_annotation:\n",
    "        if spacy_ann['type'] == 'Biological Process':\n",
    "            tp_fp = tp_fp + 1\n",
    "\n",
    "print('Spacy Precision (GR): ', tp/tp_fp)\n",
    "print('Spacy Recall (GR): ', tp/tp_fn)\n",
    "print('Spacy F1 (GR): ', 2*tp/(tp_fp + tp_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [00:00, 384323.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy Precision (RE):  0.75\n",
      "Spacy Recall (RE):  0.1477832512315271\n",
      "Spacy F1 (RE):  0.24691358024691357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# RE Evaluation\n",
    "gs_annots = json.load(open('gs_annots_6.json', 'r'))\n",
    "spacy_annots = json.load(open('spacy_annots_3.json', 'r'))\n",
    "\n",
    "tp = 0\n",
    "tp_fp = 0\n",
    "tp_fn = 0\n",
    "\n",
    "for gs_item, spacy_item in tqdm(zip(gs_annots['documents'], spacy_annots['documents'])):\n",
    "    assert gs_item['pmid'] == spacy_item['pmid']\n",
    "    gs_rels = gs_item['relations']\n",
    "    spacy_rels = spacy_item['relations']\n",
    "    for gs_rel in gs_rels:\n",
    "        for spacy_rel in spacy_rels:\n",
    "            if (gs_rel['source'].lower() == spacy_rel['source'].lower()) and (gs_rel['target'].lower() in spacy_rel['target'].lower()) and (gs_rel['relation'] == spacy_rel['relation']):\n",
    "                tp = tp + 1\n",
    "\n",
    "for gs_item in gs_annots['documents']:\n",
    "    gs_rels = gs_item['relations']\n",
    "    for gs_rel in gs_rels:\n",
    "        tp_fn = tp_fn + 1\n",
    "\n",
    "for spacy_item in spacy_annots['documents']:\n",
    "    spacy_rels = spacy_item['relations']\n",
    "    for spacy_rel in spacy_rels:\n",
    "        tp_fp = tp_fp + 1\n",
    "\n",
    "print('Spacy Precision (RE): ', tp/tp_fp)\n",
    "print('Spacy Recall (RE): ', tp/tp_fn)\n",
    "print('Spacy F1 (RE): ', 2*tp/(tp_fp + tp_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [00:00, 223467.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy Precision (RE/GR):  0.625\n",
      "Spacy Recall (RE/GR):  0.12315270935960591\n",
      "Spacy F1 (RE/GR):  0.205761316872428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# RE/GR Evaluation\n",
    "gs_annots = json.load(open('gs_annots_6.json', 'r'))\n",
    "spacy_annots = json.load(open('spacy_annots_3.json', 'r'))\n",
    "\n",
    "tp = 0\n",
    "tp_fp = 0\n",
    "tp_fn = 0\n",
    "\n",
    "for gs_item, spacy_item in tqdm(zip(gs_annots['documents'], spacy_annots['documents'])):\n",
    "    assert gs_item['pmid'] == spacy_item['pmid']\n",
    "    gs_rels = gs_item['relations']\n",
    "    spacy_rels = spacy_item['relations']\n",
    "    gs_annotation = gs_item['annotations']\n",
    "    spacy_annotation = spacy_item['annotations']\n",
    "    for gs_rel in gs_rels:\n",
    "        for spacy_rel in spacy_rels:\n",
    "            if (gs_rel['source'].lower() == spacy_rel['source'].lower()) and (gs_rel['target'].lower() in spacy_rel['target'].lower()) and (gs_rel['relation'] == spacy_rel['relation']):\n",
    "                gs_target = gs_rel['target']\n",
    "                spacy_target = spacy_rel['target']\n",
    "                for gs_ann in gs_annotation:\n",
    "                    for spacy_ann in spacy_annotation:\n",
    "                        if gs_ann['entity'].lower() == gs_target.lower() and spacy_ann['entity'].lower() == spacy_target.lower():\n",
    "                            if gs_ann['kb_id'] == spacy_ann['kb_id']:\n",
    "                                tp = tp + 1\n",
    "\n",
    "for gs_item in gs_annots['documents']:\n",
    "    gs_rels = gs_item['relations']\n",
    "    for gs_rel in gs_rels:\n",
    "        tp_fn = tp_fn + 1\n",
    "\n",
    "for spacy_item in spacy_annots['documents']:\n",
    "    spacy_rels = spacy_item['relations']\n",
    "    for spacy_rel in spacy_rels:\n",
    "        tp_fp = tp_fp + 1\n",
    "\n",
    "print('Spacy Precision (RE/GR): ', tp/tp_fp)\n",
    "print('Spacy Recall (RE/GR): ', tp/tp_fn)\n",
    "print('Spacy F1 (RE/GR): ', 2*tp/(tp_fp + tp_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of GPT-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [00:00, 139407.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Precision (NER):  0.8915094339622641\n",
      "GPT Recall (NER):  0.9264705882352942\n",
      "GPT F1: (NER) 0.9086538461538461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### NER Evaluation\n",
    "\n",
    "gs_annots = json.load(open('gs_annots_6.json', 'r'))\n",
    "gpt_annots = json.load(open('gpt4_annots_4.json', 'r'))\n",
    "\n",
    "tp = 0\n",
    "tp_fp = 0\n",
    "tp_fn = 0\n",
    "\n",
    "for gs_item, gpt_item in tqdm(zip(gs_annots['documents'], gpt_annots['documents'])):\n",
    "    assert gs_item['pmid'] == gpt_item['pmid']\n",
    "    gs_annotation = gs_item['annotations']\n",
    "    gpt_annotation = gpt_item['annotations']\n",
    "\n",
    "    for gs_ann in gs_annotation:\n",
    "        if gs_ann['type'] == 'Biological Process':\n",
    "            for gpt_ann in gpt_annotation:\n",
    "                if gpt_ann['type'] == 'Biological Process':\n",
    "                    if gs_ann['entity'].lower() in gpt_ann['entity'].lower():\n",
    "                        tp = tp + 1\n",
    "\n",
    "for gs_item in gs_annots['documents']:\n",
    "    gs_annotation = gs_item['annotations']\n",
    "    for gs_ann in gs_annotation:\n",
    "        if gs_ann['type'] == 'Biological Process':\n",
    "            tp_fn = tp_fn + 1\n",
    "\n",
    "for gpt_item in gpt_annots['documents']:\n",
    "    gpt_annotation = gpt_item['annotations']\n",
    "    for gpt_ann in gpt_annotation:\n",
    "        if gpt_ann['type'] == 'Biological Process':\n",
    "            tp_fp = tp_fp + 1\n",
    "\n",
    "print('GPT Precision (NER): ', tp/tp_fp)\n",
    "print('GPT Recall (NER): ', tp/tp_fn)\n",
    "print('GPT F1: (NER)', 2*tp/(tp_fp + tp_fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [00:00, 127845.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Precision (GR):  0.7122641509433962\n",
      "GPT Recall (GR):  0.7401960784313726\n",
      "GPT F1 (GR):  0.7259615384615384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### GR Evaluation\n",
    "\n",
    "gs_annots = json.load(open('gs_annots_6.json', 'r'))\n",
    "gpt_annots = json.load(open('gpt4_annots_4.json', 'r'))\n",
    "\n",
    "tp_fp = 0\n",
    "tp = 0\n",
    "tp_fn = 0\n",
    "\n",
    "for gs_item, gpt_item in tqdm(zip(gs_annots['documents'], gpt_annots['documents'])):\n",
    "    assert gs_item['pmid'] == gpt_item['pmid']\n",
    "    gs_annotation = gs_item['annotations']\n",
    "    gpt_annotation = gpt_item['annotations']\n",
    "\n",
    "    for gs_ann in gs_annotation:\n",
    "        if gs_ann['type'] == 'Biological Process':\n",
    "            for gpt_ann in gpt_annotation:\n",
    "                if gpt_ann['type'] == 'Biological Process':\n",
    "                    if (gs_ann['entity'].lower() in gpt_ann['entity'].lower()) and gs_ann['kb_id'] == gpt_ann['kb_id']:\n",
    "                        tp = tp + 1\n",
    "\n",
    "for gs_item in gs_annots['documents']:\n",
    "    gs_annotation = gs_item['annotations']\n",
    "    for gs_ann in gs_annotation:\n",
    "        if gs_ann['type'] == 'Biological Process':\n",
    "            tp_fn = tp_fn + 1\n",
    "\n",
    "for gpt_item in gpt_annots['documents']:\n",
    "    gpt_annotation = gpt_item['annotations']\n",
    "    for gpt_ann in gpt_annotation:\n",
    "        if gpt_ann['type'] == 'Biological Process':\n",
    "            tp_fp = tp_fp + 1\n",
    "\n",
    "print('GPT Precision (GR): ', tp/tp_fp)\n",
    "print('GPT Recall (GR): ', tp/tp_fn)\n",
    "print('GPT F1 (GR): ', 2*tp/(tp_fp + tp_fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [00:00, 134965.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Precision (RE):  0.8798076923076923\n",
      "GPT Recall (RE):  0.9014778325123153\n",
      "GPT F1 (RE):  0.8905109489051095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# RE Evaluation\n",
    "gs_annots = json.load(open('gs_annots_6.json', 'r'))\n",
    "gpt_annots = json.load(open('gpt4_annots_4.json', 'r'))\n",
    "\n",
    "tp = 0\n",
    "tp_fp = 0\n",
    "tp_fn = 0\n",
    "\n",
    "for gs_item, gpt_item in tqdm(zip(gs_annots['documents'], gpt_annots['documents'])):\n",
    "    assert gs_item['pmid'] == gpt_item['pmid']\n",
    "    gs_rels = gs_item['relations']\n",
    "    gpt_rels = gpt_item['relations']\n",
    "    for gs_rel in gs_rels:\n",
    "        for gpt_rel in gpt_rels:\n",
    "            if (gs_rel['source'].lower() == gpt_rel['source'].lower()) and (gs_rel['target'].lower() in gpt_rel['target'].lower()) and (gs_rel['relation'].lower() == gpt_rel['relation'].lower()):\n",
    "                tp = tp + 1\n",
    "\n",
    "for gs_item in gs_annots['documents']:\n",
    "    gs_rels = gs_item['relations']\n",
    "    for gs_rel in gs_rels:\n",
    "        tp_fn = tp_fn + 1\n",
    "\n",
    "for gpt_item in gpt_annots['documents']:\n",
    "    gpt_rels = gpt_item['relations']\n",
    "    for gpt_rel in gpt_rels:\n",
    "        tp_fp = tp_fp + 1\n",
    "\n",
    "print('GPT Precision (RE): ', tp/tp_fp)\n",
    "print('GPT Recall (RE): ', tp/tp_fn)\n",
    "print('GPT F1 (RE): ', 2*tp/(tp_fp + tp_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [00:00, 53248.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Precision (RE/GR):  0.7019230769230769\n",
      "GPT Recall (RE/GR):  0.7192118226600985\n",
      "GPT F1 (RE/GR):  0.7104622871046229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# RE GR Evaluation\n",
    "gs_annots = json.load(open('gs_annots_6.json', 'r'))\n",
    "gpt_annots = json.load(open('gpt4_annots_4.json', 'r'))\n",
    "\n",
    "tp = 0\n",
    "tp_fp = 0\n",
    "tp_fn = 0\n",
    "\n",
    "for gs_item, gpt_item in tqdm(zip(gs_annots['documents'], gpt_annots['documents'])):\n",
    "    assert gs_item['pmid'] == gpt_item['pmid']\n",
    "    gs_rels = gs_item['relations']\n",
    "    gpt_rels = gpt_item['relations']\n",
    "    gs_annotation = gs_item['annotations']\n",
    "    gpt_annotation = gpt_item['annotations']\n",
    "    for gs_rel in gs_rels:\n",
    "        for gpt_rel in gpt_rels:\n",
    "            if (gs_rel['source'].lower() == gpt_rel['source'].lower()) and (gs_rel['target'].lower() in gpt_rel['target'].lower()) and (gs_rel['relation'] == gpt_rel['relation']):\n",
    "                gs_target = gs_rel['target']\n",
    "                gpt_target = gpt_rel['target']\n",
    "                for gs_ann in gs_annotation:\n",
    "                    for gpt_ann in gpt_annotation:\n",
    "                        if gs_ann['entity'].lower() == gs_target.lower() and gpt_ann['entity'].lower() == gpt_target.lower():\n",
    "                            if gs_ann['kb_id'] == gpt_ann['kb_id']:\n",
    "                                tp = tp + 1\n",
    "\n",
    "for gs_item in gs_annots['documents']:\n",
    "    gs_rels = gs_item['relations']\n",
    "    for gs_rel in gs_rels:\n",
    "        tp_fn = tp_fn + 1\n",
    "\n",
    "for gpt_item in gpt_annots['documents']:\n",
    "    gpt_rels = gpt_item['relations']\n",
    "    for gpt_rel in gpt_rels:\n",
    "        tp_fp = tp_fp + 1\n",
    "\n",
    "print('GPT Precision (RE/GR): ', tp/tp_fp)\n",
    "print('GPT Recall (RE/GR): ', tp/tp_fn)\n",
    "print('GPT F1 (RE/GR): ', 2*tp/(tp_fp + tp_fn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
